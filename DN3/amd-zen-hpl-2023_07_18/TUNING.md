# AMD Zen HPL TUNING GUIDE

HPL is a complex benchmark with many options that can be tuned for the best
performance on a given system.  It is not always obvious or even knowable in
advance what all the right tuning selections should be for a given system.

It is recommended that users read the [Netlib HPL Tuning
Guide](https://netlib.org/benchmark/hpl/tuning.html) and the Netlib
description of the overall HPL
[algorithm](https://netlib.org/benchmark/hpl/algorithm.html).

AMD Zen HPL is derived from the Netlib HPL source code, so these documents
provide a good basis for understanding the HPL benchmark, and many of the
tuning parameters.


## Beyond Netlib HPL

AMD Zen HPL has a number of additional features designed to aid the
benchmarker in testing different parameters.  In addition, there are options
for various parameters that have been extended from the base Netlib versions,
giving more control to the benchmarker to fine-tune the benchmark for their
system.

### Auto-creation of starting points

A "reasonable" `HPL.dat` configuration file can be automatically generated by
AMD Zen HPL.  When invoked with the `-c` flag, AMD Zen HPL will not run the
benchmark, but will instead create a new `HPL.dat` file with a "reasonable"
configuration for the target platform, with parameters set based off of the
amount of memory on the system, the number of MPI ranks, and which AMD Zen
processor is running the benchmark.

The size of the `N` parameter will be based off of the amount of memory
of the node on which rank 0 runs.  The environment variable `HPL_RAM_CAP`
may be set to a floating point value between 0 and 1 to control how much
of the node's memory should be considered.  It defaults to 90% of the node's
memory (`HPL_RAM_CAP=0.90`).

This configuration file is not guaranteed to produce the best possible
solution, but it should provide a good starting point for selection of
parameters for a benchmarking run.

The `-o` override flag does combine with this mode, and the resulting
`HPL.dat` file (or the name specified with the `-f` flag) will reflect the
combination of the auto-selected values with the overridden values.



### Command-line support

AMD Zen HPL supports various flags on the commandline to effect the run of the
benchmark.  Of particular note while testing configurations for optimal 
performance:

   * `-f` flag to specify a filename other than `HPL.dat` to use as the
     configuration file.
   * `-o` flag allows for overriding the options specified in the `HPL.dat`
     file.
   * `-p` will have HPL print its progress during the run.
   * `-t` and `-tt` shows detailed timing information after each benchmark
     test. This can be useful to guide selection of parameters, and where to
     focus attention.
   * `-s` flag to stop the benchmark run early, after a set number of columns
     has been completed.  This is useful for testing parameters to see how
     they impact performance, without completing a full run.
   * `-S` flag similarly allows for shortening a benchmark run, but by starting
     at the specified column.



### Additional Tuning Parameters

#### Panel Broadcast

AMD Zen HPL introduces two additional options for the broadcast of factored
panels.  These are values '6' and '7', which correspond to using MPI
non-blocking broadcast (6) and a Hybrid ShMem/IBcast mode (7) which was
specifically designed for single-node HPL runs and can also work well when
multiple ranks along a row reside on the same node.

#### SWAP

The multi-rank row swapping (when P > 1) communication algorithms have been
updated and extended for additional flexibility.

In addition to SWAP 0 (bin-exch,1=long,2=mix), the Long algorithm has been
extended to allow selection of various implementations of both spread and roll
phases. For both 'spread' and 'roll', there are 3 implementations which can be
selected.  The 'spread' options may be specified by placing a 0, 1, or 2 in
the 10's column.  The 'roll' options may be specified by placing a 0, 1, or 2
in the 100's column.

  * Specify '0' in 10's position for original spread algorithm
  * Specify '1' in 10's position for spread via collectives
  * Specify '2' in 10's position for a direct spread
  * Specify '0' in 100's position for original roll algorithm
  * Specify '1' in 100's position for roll via allgatherv
  * Specify '2' in 100's position for roll via broadcasts

This applies to both '1' or '2' (mix) choices for SWAP.

In addition, a '3' value has been created which has a slightly more integrated
approach, combining direct exchanges of rows with a collective-based spread of
the U panel.  This often works well in combination with BCAST of 6 or 7, but
only when A is in Row-Major layout.

#### A matrix layout

Row 28 of HPL.dat has changed meaning.  It no longer changes the matrix layout
of the U matrix, but now does so for the A matrix.  Set this to '1' to use
Row-Major, or '0' for traditional Column-Major.

In most cases, '0' (Column Major) is best.   At very large scales, '1' (Row
Major) might provide a performance improvement in the row-swapping that is
sufficient to counter the matrix transpose overhead time.


#### MXSWP

The panel-factoring row swaps can be specified via the 'MXSWP' parameter, as
represented by a new row 32 of the HPL.dat file. The choice of '0' uses the
traditional algorithm, whereas '1' uses MPI collectives.




## Tuning process recommendation

Tuning AMD Zen HPL for a the best performance possible on given platform is
not a trivial task, no different than any other HPL implementation.  Here are
a few recommendations:

1. Determine the ratio of MPI ranks to threads.  1 MPI rank per socket or 1
   MPI rank per NUMA domain provides good results.  Use MPI process mapping
   arguments to bind processes to their appropriate domain, and fill out all
   of the cores with threads.  For example, on a 2-socket AMD EPYC 9654
   cluster:

`mpirun --map-by socket:PE=96 -x OMP_NUM_THREADS=96 -x OMP_PLACES=cores ./xhpl`

2. Invoke AMD Zen HPL with `mpirun` and allow AMD Zen HPL to auto-generate a
   starting `HPL.dat` file, by passing `-c` on the commandline.
3. Use detailed timing information printed at the ends of the run to see how
   different parameters effect parts of the benchmark. (`-t` flag to enable
   detailed timing.  Use twice to see per-rank timing information.)
4. Avoid "full" runs to speed up the process.  To benchmark how HPL behaves
   early in the run, use `-s` to stop the benchmark after the specified number
   of columns have been processed.  Early in the run, the panels are larger,
   but so are the update steps, so time is most dominated by DGEMM. Towards
   the end of the run, the remaining matrix is smaller, so other parts of the
   benchmark can dominate.  Use smaller values of `N` (half or a quarter of
   the "full" size N is often good) to see how choices of parameters impact
   the later stages of Linpack.
5. Different clusters with different interconnects can have varying
   capabilities when it comes to MPI collectives. AMD Zen HPL provides a
   number of parameters for various communication steps (BCAST, LASWP, MXSWP)
   that allow the benchmarker to use algorithms which use fully point-to-point
   communications, or collectives.  MPI implementations also provide a large
   number of tuning parameters which can be used to impact the algorithms used
   and performance of various collectives.  Broadcast and Allgather are two
   important collectives to tune.

### Parameter selection commentary

* `N` should be as large as possible to fit within the memory of the system.
  However, best performance comes when it is a multiple of the `NB` block size
  times `P` and times `Q` (least common multiple of `P` and `Q`).
* `NB` is very platform dependent, and has a very strong impact on
  performance.  AMD Zen HPL will select a good value for `NB` when run in auto
  mode, though it is possible for better values to exist.
* `NDIV` and `NBMIN` For panel factorization, best performance will be found
  when `NBMIN` evenly divides `NB`, and the same with `NDIV`.  `NDIV`
  specifies the number of divisions of the panel (blocked by size `NB`) for
  the recursive factorization, stopping when the resulting size is `NBMIN` or
  smaller.  Thus, the least amount of work to do (not necessarily the most
  performant, however) will be found when `NBMIN x NDIV == NB`, for one round
  of recursion, with no remainders.  Multiple rounds of recursion can be
  useful to shrink blocksizes further, fitting data better into cache.
* For multi-node runs, selection of `LASWP` and `BCAST` can have large impacts
  on performance.  The panel broadcast can overlap the Update phase when
  `DEPTH` >= 1, which is useful in multi-node benchmarks.  The row-swapping of
  `LASWP` cannot be overlapped with computation, and so has direct impact on
  performance.
* HPL performs best when the matrix is fairly "square" - `P` and `Q` are
  similar or equal. Very tall or very thin matrices rarely perform well.
  When it is not possible to have a "square" matrix, minimizing the ratio of
  `P` to `Q` is important. In some cases, having `P > Q` is best, and in
  others, `Q > P` performs best. Both should be attempted when the system size
  demands a non-square matrix.
